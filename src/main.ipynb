{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RETO HACKATON**: *CIENCIA DE DATOS - CAIXA BANK*\n",
    "*Carlos Cabruja - Data*\n",
    "\n",
    "## Background\n",
    "\n",
    "El IBEX 35 es el índice oficial de la bolsa española compuesto por las 35 empresas más negociadas del mercado. Este índice nos muestra en tiempo real si los precios en bolsa están subiendo o bajando, por lo que permite medir el comportamiento de este conjunto de acciones.\n",
    "\n",
    "El IBEX35 sirve como punto de referencia para los inversores del mercado español. La rentabilidad de este índice es el objetivo a batir por los gestores.\n",
    "\n",
    "Por lo tanto, la modelización de las dinámicas de este tipo de índices resultan esenciales para la toma de decisiones por parte de todas las entidades bursátiles.\n",
    "\n",
    "## Reto\n",
    "\n",
    "1. Desarrolla un modelo predictivo que permita predecir la variable target (si el precio de cierre del IBEX35 será superior o inferior al precio de cierre actual).\n",
    "\n",
    "Para ello deberas entrenar tu modelo con los datos de training (si también se usan los tweets se sumaran 100 puntos) e introducir como input de tu modelo el dataset test_x para realizar las predicciones.\n",
    "\n",
    "2. Crea un breve documento (máx. 2 páginas) o presentación (máx. 4 slides) explicando la solución que has empleado y porque la has empleado.\n",
    "\n",
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import datetime as dt\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import multiprocessing\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recopilación y análisis de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datasets a trabajar\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test_x.csv')\n",
    "tweets = pd.read_csv('data/tweets_from2015_#Ibex35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train # visualizamos los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test # visualizamos los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets # visualizamos los datos de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estan correctamente cargados csv, ahora vamos su tipo y si presentan valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habrá que hacer tratamiento de nulls en los datos y convertir la columna **Date** en datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece ya encontrase limpia y lista para ser procesada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer **tweetdate** un Date para que pueda hacer merge con nuestro dataframe de train, y tratamiento de nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuantos nulls tiene tweets?\n",
    "tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filas con nulls en tweetDate\n",
    "tweets[tweets.tweetDate.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay nada, eliminamos...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar filas con nulls en tweetDate\n",
    "tweets = tweets.dropna(subset=['tweetDate'])\n",
    "\n",
    "# filas con nulls en text\n",
    "tweets[tweets.text.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tampoco son relevantes los tweets que no tienen fecha de publicación, ni texto de los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar todos los nulls\n",
    "tweets = tweets.dropna()\n",
    "tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir Date a datetime YYYY-MM-DD\n",
    "try:\n",
    "    tweets['tweetDate'] = pd.to_datetime(tweets['tweetDate'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como **tweetdate** está contaminada con texto la vamos a tratar fila por fila. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tweets['tweetDate']:\n",
    "    lista_i = str(i).split(' ')\n",
    "    if len(lista_i) != 6: # si no tiene 6 elementos (YYYY-MM-DD HH:MM:SS)\n",
    "        # eliminar filas con fechas mal formateadas\n",
    "        tweets = tweets.drop(tweets[tweets['tweetDate'] == i].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir Date a datetime YYYY-MM-DD\n",
    "tweets['tweetDate'] = pd.to_datetime(tweets['tweetDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renombrar tweetDate a Date\n",
    "tweets = tweets.rename(columns={'tweetDate': 'Date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la columna handle no nos sirve ya que no daremos importancia a quién ha escrito el tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar handle de los tweets\n",
    "tweets = tweets.drop(['handle'], axis=1)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos la columna sentimiento, con la libreria de Sentiment Analysis de NLTK.\n",
    "\n",
    "**Análisis de sentimiento VADER:**\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) es una herramienta de análisis de sentimientos basada en reglas y léxico que está específicamente en sintonía con los sentimientos expresados ​​en las redes sociales. VADER usa una combinación de un léxico de sentimientos es una lista de características léxicas (por ejemplo, palabras) que generalmente se etiquetan según su orientación semántica como positivas o negativas. VADER no solo informa sobre la puntuación de positividad y negatividad, sino que también nos informa sobre qué tan positivo o negativo es un sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer() # instanciamos el analizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'] = tweets['text'].apply(lambda x: sentiment.polarity_scores(x)['compound'])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar text\n",
    "tweets = tweets.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, a fin de hacer el merge con train, trataremos los datos para que sea solo una fecha.\n",
    "\n",
    "Para los tweets hechos en una misma fecha pero distinta hora, se hará una media de los sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir Date a datetime YYYY-MM-DD\n",
    "tweets['Date'] = pd.to_datetime(tweets['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar Dates repetidos con el mismo sentimiento y fecha\n",
    "tweets = tweets.drop_duplicates()\n",
    "\n",
    "# hay alguna fecha repetida con distinto sentimiento?\n",
    "tweets.groupby(['Date', 'sentiment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacamos las fechas repetidas con distinto sentimiento\n",
    "temp_df = tweets.groupby(['Date']).count()\n",
    "lista_fechas = temp_df[temp_df['sentiment'] > 1].index\n",
    "\n",
    "# restamos el indice de sentimiento en las fechas repetidas\n",
    "for i in lista_fechas:\n",
    "    # sacamos las filas repetidas\n",
    "    temp_df = tweets[tweets['Date'] == i]\n",
    "    # restamos el indice de sentimiento en las fechas repetidas\n",
    "    sentiment = temp_df['sentiment'].mean()\n",
    "    # actualizamos el sentimiento en las filas repetidas\n",
    "    tweets.loc[tweets['Date'] == i, 'sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos las filas repetidas\n",
    "tweets = tweets.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos nuestra tabla tweets limpia, así que hacemos el merge con train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer train Date a datetime\n",
    "train['Date'] = pd.to_datetime(train['Date']).dt.date\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge de train y tweets\n",
    "train_tweets = pd.merge(train, tweets, on='Date')\n",
    "train_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a limpiar nuestros datos de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra los nulos en open\n",
    "train_tweets[train_tweets.Open.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contiene ninguna información relevante, por lo que la vamos a eliminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = train_tweets.dropna()\n",
    "train_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como último paso vamos a categorizar la influencia de los tweets en neutral si el sentimiento es 0, positivo si es mayor a 0 y negativo si es menor a 0.\n",
    "\n",
    "Esto lo harémos porque ya la influencia del tweet está documentada como importante que son más de 2 likes y 2 retweets, entonces aunque VADER nos dé un valor muy cercano al 0 (neutral) lo que queremos es determinar si la influencia de los tweets aporta valor a nuestro modelo, así que valoraremos esa diferencia del neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir sentiment a neutral, positivo y negativo\n",
    "train_tweets['sentiment'] = train_tweets['sentiment'].apply(lambda x: 'neutral' if x == 0 else ('positivo' if x > 0 else 'negativo'))\n",
    "train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que la mayoría sea neutral, nos aportará una mayor información para determinar si esos tweets que valoramos como negativos o positivos indiferentemente de su distancia a la neutralidad tienen alguna influencia en el modelo y estudiar un modelo que tenga en cuenta cuanto porcentaje de negatividad o positividad tiene influencia en el indice IBEX35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos los datos en un pickle\n",
    "train_tweets.to_pickle('data/train_tweets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hacemos el merge de los tweets con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer test Date a datetime\n",
    "test['Date'] = pd.to_datetime(test['Date']).dt.date\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge de test y tweets\n",
    "test_tweets = pd.merge(test, tweets, on='Date')\n",
    "test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir sentiment a neutral, positivo y negativo\n",
    "test_tweets['sentiment'] = test_tweets['sentiment'].apply(lambda x: 'neutral' if x == 0 else ('positivo' if x > 0 else 'negativo'))\n",
    "test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos los datos en un pickle\n",
    "test_tweets.to_pickle('data/test_tweets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir sentiment a dummies\n",
    "train_tweets = pd.get_dummies(train_tweets, columns=['sentiment'])\n",
    "train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = pd.get_dummies(test_tweets, columns=['sentiment'])\n",
    "test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estandarizacón de variables númericas\n",
    "num_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
    "\n",
    "# aplicar standarScaler a las variables númericas\n",
    "scaler = StandardScaler()\n",
    "train_tweets[num_cols] = scaler.fit_transform(train_tweets[num_cols])\n",
    "\n",
    "train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets[num_cols] = scaler.transform(test_tweets[num_cols])\n",
    "prep_test = test_tweets.copy()\n",
    "prep_test = prep_test.drop(['test_index'], axis=1)\n",
    "prep_test = prep_test.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_tweets.drop(['Target'], axis=1).set_index('Date')\n",
    "y = train_tweets['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelización\n",
    "\n",
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de hiperparámetros evaluados\n",
    "# ==============================================================================\n",
    "param_grid = {'n_estimators': [150],\n",
    "              'max_depth'   : [None, 3, 10, 20],\n",
    "              'criterion'   : ['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "# Búsqueda por grid search con validación cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = RandomForestClassifier(random_state = 123),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'accuracy',\n",
    "        n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "        cv         = RepeatedKFold(n_splits=5, n_repeats=3, random_state=123), \n",
    "        refit      = True,\n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "       )\n",
    "\n",
    "grid.fit(X = X, y = y)\n",
    "\n",
    "# Resultados\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros por validación cruzada\n",
    "# ==============================================================================\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "print(\"----------------------------------------\")\n",
    "print(grid.best_params_, \":\", grid.best_score_, grid.scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de hiperparámetros evaluados\n",
    "# ==============================================================================\n",
    "param_grid = {'n_estimators'  : [50, 100, 500, 1000],\n",
    "              'max_features'  : ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth'     : [None, 1, 3, 5, 10, 20],\n",
    "              'subsample'     : [0.5, 1],\n",
    "              'learning_rate' : [0.001, 0.01, 0.1]\n",
    "             }\n",
    "\n",
    "# Búsqueda por grid search con validación cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = GradientBoostingClassifier(random_state=123),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'accuracy',\n",
    "        n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "        cv         = RepeatedKFold(n_splits=3, n_repeats=1, random_state=123), \n",
    "        refit      = True,\n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "       )\n",
    "\n",
    "grid.fit(X = X, y = y)\n",
    "\n",
    "# Resultados\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros por validación cruzada\n",
    "# ==============================================================================\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "print(\"----------------------------------------\")\n",
    "print(grid.best_params_, \":\", grid.best_score_, grid.scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting = grid.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4888b20ad2e0830b2b3289a265d49b7b8db6bb4be38e5df533fd94a4ada827bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('the_bridge_22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
